{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Tarea regresion",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLbY9HJFITIx9zC2UyorH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/econb/data-science-mae/blob/main/1_Tarea_regresion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qiROs24GjQD"
      },
      "source": [
        "<font color='gray'>Eduardo Contreras B.  \r\n",
        "Temas Avanzados en Estadística: Ciencia de Datos  \r\n",
        "Universidad Nacional de Colombia</font>\r\n",
        "\r\n",
        "<h1><center><font color='blue'>Entendiendo Jupyter Lab, Github, Python y Regresión Lineal</font></center></h1>\r\n",
        "\r\n",
        "# Introducción\r\n",
        "La regresión lineal es un método que permite...\r\n",
        "\r\n",
        "\r\n",
        "# Historia\r\n",
        "The earliest form of regression was the method of least squares, which was published by Legendre in 1805,[4] and by Gauss in 1809.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# Algoritmo\r\n",
        "$ Y \\sim N_n(XB, \\sigma^2I)$\r\n",
        "\r\n",
        "$L(B) = \\frac{1}{(2\\pi)^{n/2}} \\frac{1}{\\left | \\sigma^2I \\right |^{-1/2}} e^{-\\frac{1}{2\\sigma^2}(Y-XB)^T(Y-XB)} $\r\n",
        "\r\n",
        "$logL(B) = -\\frac{n}{2}log(2\\pi) -\\frac{1}{2}log\\left| \\sigma^2I \\right| -\\frac{1}{2\\sigma^2}(Y-XB)^T(Y-XB)$\r\n",
        "\r\n",
        "$\\frac{\\partial logL(B)}{\\partial B} = -\\frac{1}{2}\\frac{2}{\\sigma^2}(X^TXB-X^TY)$\r\n",
        "\r\n",
        "$\\frac{\\partial logL(B)}{\\partial B} = 0$\r\n",
        "\r\n",
        "$-\\frac{1}{\\sigma^2}(X^TXB-X^TY) = 0$\r\n",
        "\r\n",
        "$\\hat{B}=(X^TX)^{-1}X^TY$\r\n",
        "\r\n",
        "\r\n",
        "# Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL0olmzrL-kY"
      },
      "source": [
        "#Read data\r\n",
        "import statsmodels.api as sm\r\n",
        "from sklearn import datasets\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "data = datasets.load_boston()\r\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names) #data.data is a numpy ndarray\r\n",
        "Y = pd.DataFrame(data.target, columns=[\"MEDV\"]) #data.target is a numpy ndarray"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO6J3SZj9Y07",
        "outputId": "73472641-91ec-445e-e3ce-2334fbb49477"
      },
      "source": [
        "#Linear Regression with statsmodels\r\n",
        "fitsm = sm.OLS(Y, sm.add_constant(X)).fit()\r\n",
        "print(fitsm.summary())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                   MEDV   R-squared:                       0.741\n",
            "Model:                            OLS   Adj. R-squared:                  0.734\n",
            "Method:                 Least Squares   F-statistic:                     108.1\n",
            "Date:                Sun, 14 Mar 2021   Prob (F-statistic):          6.72e-135\n",
            "Time:                        00:21:09   Log-Likelihood:                -1498.8\n",
            "No. Observations:                 506   AIC:                             3026.\n",
            "Df Residuals:                     492   BIC:                             3085.\n",
            "Df Model:                          13                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "const         36.4595      5.103      7.144      0.000      26.432      46.487\n",
            "CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043\n",
            "ZN             0.0464      0.014      3.382      0.001       0.019       0.073\n",
            "INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141\n",
            "CHAS           2.6867      0.862      3.118      0.002       0.994       4.380\n",
            "NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262\n",
            "RM             3.8099      0.418      9.116      0.000       2.989       4.631\n",
            "AGE            0.0007      0.013      0.052      0.958      -0.025       0.027\n",
            "DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084\n",
            "RAD            0.3060      0.066      4.613      0.000       0.176       0.436\n",
            "TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005\n",
            "PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696\n",
            "B              0.0093      0.003      3.467      0.001       0.004       0.015\n",
            "LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425\n",
            "==============================================================================\n",
            "Omnibus:                      178.041   Durbin-Watson:                   1.078\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n",
            "Skew:                           1.521   Prob(JB):                    8.84e-171\n",
            "Kurtosis:                       8.281   Cond. No.                     1.51e+04\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO5eVcFT-cfO",
        "outputId": "22b82fe9-6ba8-4cde-d50d-fe0abb8e69e9"
      },
      "source": [
        "#Linear Regression with sklearn\r\n",
        "from sklearn import feature_selection\r\n",
        "from sklearn import linear_model\r\n",
        "\r\n",
        "modelskl = linear_model.LinearRegression()\r\n",
        "fitskl = modelskl.fit(X,Y)\r\n",
        "print(\"Coefficients\")\r\n",
        "print(fitskl.coef_)\r\n",
        "\r\n",
        "fvals, pvals = sklearn.feature_selection.f_regression(X, Y)\r\n",
        "print(\"pvalues\")\r\n",
        "print(pvals)\r\n",
        "\r\n",
        "print(\"R-squared\")\r\n",
        "predskl = modelskl.predict(X)\r\n",
        "sklearn.metrics.r2_score(Y, predskl)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients\n",
            "[[-1.08011358e-01  4.64204584e-02  2.05586264e-02  2.68673382e+00\n",
            "  -1.77666112e+01  3.80986521e+00  6.92224640e-04 -1.47556685e+00\n",
            "   3.06049479e-01 -1.23345939e-02 -9.52747232e-01  9.31168327e-03\n",
            "  -5.24758378e-01]]\n",
            "pvalues\n",
            "[1.17398708e-19 5.71358415e-17 4.90025998e-31 7.39062317e-05\n",
            " 7.06504159e-24 2.48722887e-74 1.56998221e-18 1.20661173e-08\n",
            " 5.46593257e-19 5.63773363e-29 1.60950948e-34 1.31811273e-14\n",
            " 5.08110339e-88]\n",
            "R-squared\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7406426641094095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaw09zIvdbb_",
        "outputId": "90abb8bc-7863-440b-98f0-8946eba3a344"
      },
      "source": [
        "#Linear Regresion by computing the equation\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "Xnp = np.asmatrix(sm.add_constant(X).values)\r\n",
        "Ynp = np.asmatrix(Y.values)\r\n",
        "B = np.matmul(np.linalg.inv(np.matmul(Xnp.T,Xnp)), np.matmul(Xnp.T,Ynp))\r\n",
        "B"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 3.64594884e+01],\n",
              "        [-1.08011358e-01],\n",
              "        [ 4.64204584e-02],\n",
              "        [ 2.05586264e-02],\n",
              "        [ 2.68673382e+00],\n",
              "        [-1.77666112e+01],\n",
              "        [ 3.80986521e+00],\n",
              "        [ 6.92224640e-04],\n",
              "        [-1.47556685e+00],\n",
              "        [ 3.06049479e-01],\n",
              "        [-1.23345939e-02],\n",
              "        [-9.52747232e-01],\n",
              "        [ 9.31168327e-03],\n",
              "        [-5.24758378e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVXZKLvSMK4-"
      },
      "source": [
        "# Conclusiones\r\n",
        "\r\n",
        "\r\n",
        "# Referencias\r\n",
        "https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9\r\n"
      ]
    }
  ]
}